var documenterSearchIndex = {"docs":
[{"location":"man/logger_guide/#Logging","page":"Logging","title":"Logging","text":"","category":"section"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"The function custom_logger is a wrapper over the Logging.jl and LoggingExtras.jl libraries. I made them such that I could fine tune the type of log I use repeatedly across projects. ","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"The things I find most useful:","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"four different log files for each different level of logging from error to debug\npretty (to me) formatting for stdout but also an option to have log4j style formatting in the files\nfiltering out messages of verbose packages (TranscodingStreams, etc...) which sometimes slows down julia because of excessive logging.","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"There are still a few things that might be useful down the line: (1) a catch-all log file where filters do not apply; (2) filtering out specific functions of packages; ","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"Overall this is working fine for me.","category":"page"},{"location":"man/logger_guide/#Basic-usage","page":"Logging","title":"Basic usage","text":"","category":"section"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"Say at the beginning of a script you would have something like:","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"using BazerUtils\ncustom_logger(\"/tmp/log_test\"; \n    filtered_modules_all=[:StatsModels, :TranscodingStreams, :Parquet2], \n    create_log_files=true, \n    overwrite=true, \n    log_format = :log4j);\n  \n┌ Info: Creating four different files for logging ...\n│  ⮑  /tmp/log_test_error.log\n│      /tmp/log_test_warn.log\n│      /tmp/log_test_info.log\n└      /tmp/log_test_debug.log","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"The REPL will see all messages above debug level:","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"> @error \"This is an error level message\"\n┌ [08:28:08 2025-02-12] ERROR |  @ Main[REPL[17]:1]\n└ This is an error level message\n\n> @warn \"This is an warn level message\"\n┌ [08:28:08 2025-02-12] WARN  |  @ Main[REPL[18]:1]\n└ This is an warn level message\n\n> @info \"This is an info level message\"\n┌ [08:28:08 2025-02-12] INFO  |  @ Main[REPL[19]:1]\n└ This is an info level message\n\n> @debug \"This is an debug level message\"\n","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"Then each of the respective log-levels will be redirected to the individual files and if the log4j option was specified they will look like something like this","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"2025-02-12 08:28:08 ERROR Main[REPL[17]:1] - This is an error level message\n2025-02-12 08:28:08 WARN  Main[REPL[18]:1] - This is an warn level message\n2025-02-12 08:28:08 INFO  Main[REPL[19]:1] - This is an info level message\n2025-02-12 08:28:08 DEBUG Main[REPL[20]:1] - This is an debug level message","category":"page"},{"location":"man/logger_guide/#Options","page":"Logging","title":"Options","text":"","category":"section"},{"location":"man/logger_guide/#Formatting","page":"Logging","title":"Formatting","text":"","category":"section"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"The log_format is log4j by default (only for the files).  The only other option for now is pretty which uses the format I wrote for the REPL; note that it is a little cumbersome for files especially since you have to make sure your editor has the ansi interpreter on. ","category":"page"},{"location":"man/logger_guide/#Files","page":"Logging","title":"Files","text":"","category":"section"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"The default is to create one file for each level.  There is an option to only create one file for each level and keep things a little tidier in your directories:","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"> custom_logger(\"/tmp/log_test\";  \n    create_log_files=false, overwrite=true, log_format = :log4j);\n\n> @error \"This is an error level message\" \n> @warn \"This is an warn level message\"\n> @info \"This is an info level message\"\n> @debug \"This is an debug level message\"","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"And then the file /tmp/log_test has the following:","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"2025-02-12 08:37:29 ERROR Main[REPL[22]:1] - This is an error level message\n2025-02-12 08:37:29 WARN  Main[REPL[23]:1] - This is an warn level message\n2025-02-12 08:37:29 INFO  Main[REPL[24]:1] - This is an info level message\n2025-02-12 08:37:29 DEBUG Main[REPL[25]:1] - This is an debug level message","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"Now imagine you want to keep the same log file but for a different script.  You can use the same logger option with the overwrite=false option:","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"> custom_logger(\"/tmp/log_test\";  \n    create_log_files=false, overwrite=false, log_format = :log4j);\n\n> @error \"This is an error level message from a different script and new logger\" ","category":"page"},{"location":"man/logger_guide/#Filtering","page":"Logging","title":"Filtering","text":"","category":"section"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"filtered_modules_specific::Vector{Symbol}=nothing: which modules do you want to filter out of logging (only for info and stdout) Some packages just write too much log ... filter them out but still be able to check them out in other logs\nfiltered_modules_all::Vector{Symbol}=nothing: which modules do you want to filter out of logging (across all logs)  Examples could be TranscodingStreams (noticed that it writes so much to logs that it sometimes slows down I/O)","category":"page"},{"location":"man/logger_guide/#Other","page":"Logging","title":"Other","text":"","category":"section"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"For log4j I do modify the message string to fit on one line.  You will find that the \"\\n\" is now replaced by \" | \"; I guess I could have an option for which character delimitates lines, but this seems too fussy.","category":"page"},{"location":"man/logger_guide/","page":"Logging","title":"Logging","text":"I am trying to have a path shortener that would allow to reduce the path of the function to a fixed size. The cost is that path will no longer be \"clickable\" but we would keep things tidy as messages will all start at the same column. (see the shorten_path_str function).","category":"page"},{"location":"lib/internals/#Package-Internals","page":"Package Internals","title":"Package Internals","text":"","category":"section"},{"location":"lib/internals/#BazerUtils-Module","page":"Package Internals","title":"BazerUtils Module","text":"","category":"section"},{"location":"lib/internals/#BazerUtils._dict_of_json3-Tuple{JSON3.Object}","page":"Package Internals","title":"BazerUtils._dict_of_json3","text":"_dict_of_json3(obj::JSON3.Object) -> Dict{Symbol, Any}\n\nRecursively convert a JSON3.Object (from JSON3.jl) into a standard Julia Dict with Symbol keys.\n\nThis function traverses the input JSON3.Object, converting all keys to Symbol and recursively converting any nested JSON3.Object values. Non-object values are left unchanged.\n\nArguments\n\nobj::JSON3.Object: The JSON3 object to convert.\n\nReturns\n\nDict{Symbol, Any}: A Julia dictionary with symbol keys and values converted recursively.\n\nNotes\n\nThis function is intended for internal use and is not exported.\nUseful for converting parsed JSON3 objects into standard Julia dictionaries for easier manipulation.\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#BazerUtils.reformat_msg-Tuple{Any}","page":"Package Internals","title":"BazerUtils.reformat_msg","text":"reformat_msg\n# we view strings as simple and everything else as complex\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#BazerUtils.shorten_path_str-Tuple{AbstractString}","page":"Package Internals","title":"BazerUtils.shorten_path_str","text":"shorten_path_str(path::AbstractString; max_length::Int=40, strategy::Symbol=:truncate_middle)\n\nShorten a file path string to a specified maximum length using various strategies.\n\nArguments\n\npath::AbstractString: The input path to be shortened\nmax_length::Int=40: Maximum desired length of the output path\nstrategy::Symbol=:truncate_middle: Strategy to use for shortening. Options:\n:no: Return path unchanged\n:truncate_middle: Truncate middle of path components while preserving start/end\n:truncate_to_last: Keep only the last n components of the path\n:truncate_from_right: Progressively remove characters from right side of components\n:truncate_to_unique: Reduce components to unique prefixes\n\nReturns\n\nString: The shortened path\n\nExamples\n\n# Using different strategies\njulia> shorten_path_str(\"/very/long/path/to/file.txt\", max_length=20)\n\"/very/…/path/to/file.txt\"\n\njulia> shorten_path_str(\"/usr/local/bin/program\", strategy=:truncate_to_last, max_length=20)\n\"/bin/program\"\n\njulia> shorten_path_str(\"/home/user/documents/very_long_filename.txt\", strategy=:truncate_middle)\n\"/home/user/doc…ents/very_…name.txt\"\n\n\n\n\n\n","category":"method"},{"location":"man/read_jsonl/#Working-with-JSON-Lines-Files","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"","category":"section"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"JSON Lines (JSONL) is a convenient format for storing structured data that may be processed one record at a time. Each line is a valid JSON value, separated by a newline character. This format is ideal for large datasets and streaming applications.","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"For more details, see jsonlines.org.","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"","category":"page"},{"location":"man/read_jsonl/#What-is-JSON-Lines?","page":"Working with JSON Lines Files","title":"What is JSON Lines?","text":"","category":"section"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"UTF-8 Encoding: Files must be UTF-8 encoded. Do not include a byte order mark (BOM).\nOne JSON Value Per Line: Each line is a valid JSON value (object, array, string, number, boolean, or null). Blank lines are ignored.\nLine Separator: Each line ends with \\n (or \\r\\n). The last line may or may not end with a newline.","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"Example:","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"{\"name\": \"Alice\", \"score\": 42}\n{\"name\": \"Bob\", \"score\": 17}\n[1, 2, 3]\n\"hello\"\nnull","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"","category":"page"},{"location":"man/read_jsonl/#Reading-JSON-Lines-Files","page":"Working with JSON Lines Files","title":"Reading JSON Lines Files","text":"","category":"section"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"You can use the read_jsonl and stream_jsonl functions to read JSONL files or streams.","category":"page"},{"location":"man/read_jsonl/#read_jsonl","page":"Working with JSON Lines Files","title":"read_jsonl","text":"","category":"section"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"Reads the entire file or stream into memory and returns a vector of parsed JSON values.","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"using BazerUtils\nimport JSON3\ndata = read_jsonl(\"data.jsonl\")\n# or from an IOBuffer\nbuf = \ndata = read_jsonl(IOBuffer(\"{\\\"a\\\": 1}\\n{\\\"a\\\": 2}\\n\"))\ndata = read_jsonl(IOBuffer(\"{\\\"a\\\": 1}\\n{\\\"a\\\": 2}\\n\"); dict_of_json=true)","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"Arguments: source::Union{AbstractString, IO}\nReturns: Vector of parsed JSON values\nNote: Loads all data into memory. For large files, use stream_jsonl.","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"","category":"page"},{"location":"man/read_jsonl/#stream_jsonl","page":"Working with JSON Lines Files","title":"stream_jsonl","text":"","category":"section"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"Creates a lazy iterator (Channel) that yields one parsed JSON value at a time, without loading the entire file into memory.","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"stream = stream_jsonl(IOBuffer(\"{\\\"a\\\": 1}\\n{\\\"a\\\": 2}\\n\"))\ndata = collect(stream)\nBazerUtils._dict_of_json3.(data)\n\nstream = stream_jsonl(IOBuffer(\"{\\\"a\\\": 1}\\n{\\\"a\\\": 2}\\n[1,2,3]\"))\ncollect(stream) # error because types of vector elements are not all JSON3.Object{}\nstream = stream_jsonl(IOBuffer(\"{\\\"a\\\": 1}\\n{\\\"a\\\": 2}\\n[1,2,3]\"), T=Any)\ncollect(stream) # default to Vector{Any}\n\nstream = stream_jsonl(IOBuffer(\"[4,5,6]\\n[1,2,3]\"), T= JSON3.Array{})\ncollect(stream)\nstream = stream_jsonl(IOBuffer(\"4\\n1\"), T=Int)\ncollect(stream)","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"Allows iterators","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"first10 = collect(Iterators.take(stream_jsonl(\"data.jsonl\"), 10)) # Collect the first 10 records\n# see tests for other iterators ...","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"Arguments: source::Union{AbstractString, IO}\nReturns: Channel (iterator) of parsed JSON values\nNote: Ideal for large files and streaming workflows.","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"","category":"page"},{"location":"man/read_jsonl/#Writing-JSON-Lines-Files","page":"Working with JSON Lines Files","title":"Writing JSON Lines Files","text":"","category":"section"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"Use write_jsonl to write an iterable of JSON-serializable values to a JSONL file.","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"write_jsonl(\"out.jsonl\", [Dict(\"a\"=>1), Dict(\"b\"=>2)])\nwrite_jsonl(\"out.jsonl.gz\", (Dict(\"i\"=>i) for i in 1:100); compress=true)","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"Arguments: \nfilename::AbstractString\ndata: iterable of JSON-serializable values\ncompress::Bool=false: write gzip-compressed if true or filename ends with .gz\nReturns: The filename","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"","category":"page"},{"location":"man/read_jsonl/#Example:-Roundtrip-with-IOBuffer","page":"Working with JSON Lines Files","title":"Example: Roundtrip with IOBuffer","text":"","category":"section"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"Note that there is no stable roundtrip between read and write, because of the way JSON3 processes record into dictionaries and even when we add the dict flag it is Symbol => Any","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"data_string = [Dict(\"a\"=>1), Dict(\"b\"=>2)]\ndata_symbol = [Dict(:a=>1), Dict(:b=>2)]\n\nfunction roundtrip(data)\n    buf = IOBuffer()\n    for obj in data\n        JSON3.write(buf, obj)\n        write(buf, '\\n')\n    end\n    seekstart(buf)\n    return read_jsonl(buf; dict_of_json=true)\nend\n\nroundtrip(data_string) == data_string\nroundtrip(data_symbol) == data_symbol","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"","category":"page"},{"location":"man/read_jsonl/#See-Also","page":"Working with JSON Lines Files","title":"See Also","text":"","category":"section"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"JSON3.jl: Fast, flexible JSON parsing and serialization for Julia.\nCodecZlib.jl: Gzip compression support.","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"","category":"page"},{"location":"man/read_jsonl/","page":"Working with JSON Lines Files","title":"Working with JSON Lines Files","text":"For more advanced usage and performance tips, see the main documentation and function docstrings.","category":"page"},{"location":"lib/public/#Public-Interface","page":"Public Interface","title":"Public Interface","text":"","category":"section"},{"location":"lib/public/#BazerUtils-Module","page":"Public Interface","title":"BazerUtils Module","text":"","category":"section"},{"location":"lib/public/#BazerUtils.custom_logger-Tuple{BazerUtils.LogSink}","page":"Public Interface","title":"BazerUtils.custom_logger","text":"custom_logger(filename; kw...)\n\nArguments\n\nfilename::AbstractString: base name for the log files\noutput_dir::AbstractString=./log/: name of directory where log files are written\nfiltered_modules_specific::Vector{Symbol}=nothing: which modules do you want to filter out of logging (only for info and stdout) Some packages just write too much log ... filter them out but still be able to check them out in other logs\nfiltered_modules_all::Vector{Symbol}=nothing: which modules do you want to filter out of logging (across all logs)  Examples could be TranscodingStreams (noticed that it writes so much to logs that it sometimes slows down I/O)\nfile_loggers::Union{Symbol, Vector{Symbol}}=[:error, :warn, :info, :debug]: which file logger to register \nlog_date_format::AbstractString=\"yyyy-mm-dd\": time stamp format at beginning of each logged lines for dates\nlog_time_format::AbstractString=\"HH:MM:SS\": time stamp format at beginning of each logged lines for times\ndisplaysize::Tuple{Int,Int}=(50,100): how much to show on log (same for all logs for now!)\nlog_format::Symbol=:log4j: how to format the log files; I have added an option for pretty (all or nothing for now)\nlog_format_stdout::Symbol=:pretty: how to format the stdout; default is pretty\noverwrite::Bool=false: do we overwrite previously created log files\n\nThe customlogger function creates four files in `outputdirfor four different levels of logging:     from least to most verbose:filename.info.log.jl,filename.warn.log.jl,filename.debug.log.jl,filename.full.log.jlThe debug logging offers the option to filter messages from specific packages (some packages are particularly verbose) using thefilter` optional argument The full logging gets all of the debug without any of the filters. Info and warn log the standard info and warning level logging messages.\n\nNote that the default overwrites old log files (specify overwrite=false to avoid this).\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#BazerUtils.read_jsonl-Tuple{IO}","page":"Public Interface","title":"BazerUtils.read_jsonl","text":"read_jsonl(source::Union{AbstractString, IO}; dict_of_json::Bool=false) -> Vector\n\nRead a JSON Lines (.jsonl) file or stream and return all records as a vector.\n\nThis function reads the entire file or IO stream into memory at once, parsing each line as a separate JSON value. Empty lines are automatically skipped.\n\nArguments\n\nsource::Union{AbstractString, IO}: Path to the JSON Lines file to read, or an IO stream (e.g., IOBuffer, file handle).\ndict_of_json::Bool=false: If true and the parsed type is JSON3.Object, convert each record to a Dict{Symbol,Any}.\n\nReturns\n\nVector: A vector containing all parsed JSON values from the file or stream.\n\nExamples\n\n# Read all records from a JSONL file\ndata = read_jsonl(\"data.jsonl\")\n\n# Read from an IOBuffer\nbuf = IOBuffer(\"{\"a\":1}\n{\"a\":2}\n\")\ndata = read_jsonl(buf)\n\n# Convert JSON3.Object records to Dict\ndata = read_jsonl(\"data.jsonl\"; dict_of_json=true)\n\n# Access individual records\nfirst_record = data[1]\nprintln(\"First record ID: \", first_record.id)\n\nNotes\n\nThis function loads all data into memory, so it may not be suitable for very large files.\nFor large files, consider using stream_jsonl() for streaming processing.\nThe function will throw an error if the JSON on any line is malformed.\nThe path must refer to an existing regular file.\nIf dict_of_json=true, all records must be of type JSON3.Object.\n\nSee Also\n\nstream_jsonl: For memory-efficient streaming of large JSONL files.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#BazerUtils.stream_jsonl-Tuple{IO}","page":"Public Interface","title":"BazerUtils.stream_jsonl","text":"stream_jsonl(source::Union{AbstractString, IO}; T::Type=JSON3.Object{}) -> Channel\n\nCreate a lazy iterator (Channel) for reading JSON Lines files record by record.\n\nThis function returns a Channel that yields JSON objects one at a time without loading the entire file into memory. This is memory-efficient for processing large JSONL files. Each parsed record is checked to match the specified type T (default: JSON3.Object{}). If a record does not match T, an error is thrown.\n\nArguments\n\nsource::Union{AbstractString, IO}: Path to the JSON Lines file to read, or an IO stream (e.g., IOBuffer, file handle).\nT::Type=JSON3.Object{}: The expected type for each parsed record. Use T=Any to allow mixed types.\n\nReturns\n\nChannel{T}: A channel that yields parsed JSON objects one at a time.\n\nExamples\n\n# Process records one at a time (memory efficient)\nfor record in stream_jsonl(\"large_file.jsonl\")\n    println(\"Processing record: \", record.id)\nend\n\n# Collect first N records\nfirst_10 = collect(Iterators.take(stream_jsonl(\"data.jsonl\"), 10))\n\n# Filter and process\nfiltered_records = [r for r in stream_jsonl(\"data.jsonl\") if r.score > 0.5]\n\n# Stream from an IOBuffer\nbuf = IOBuffer(\"{\"a\":1}\n{\"a\":2}\n\")\nfor record in stream_jsonl(buf)\n    @show record\nend\n\n# Allow mixed types\nfor record in stream_jsonl(\"data.jsonl\"; T=Any)\n    @show record\nend\n\nNotes\n\nThis is a lazy iterator: records are only read and parsed when requested.\nMemory usage remains constant regardless of file size.\nEmpty lines are automatically skipped.\nThe Channel is automatically closed when the file or stream is fully read or an error occurs.\nIf JSON parsing fails on any line, the Channel will close and propagate the error.\nFor file paths, the file remains open for the lifetime of the channel.\nFor IO streams, the user is responsible for keeping the IO open while consuming the channel.\nIf a parsed record does not match T, an error is thrown. Use T=Any to allow mixed types.\n\nSee Also\n\nread_jsonl: For loading entire JSONL files into memory at once.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#BazerUtils.write_jsonl-Tuple{AbstractString, Any}","page":"Public Interface","title":"BazerUtils.write_jsonl","text":"write_jsonl(filename, data; compress=false)\n\nWrite an iterable of JSON-serializable values to a JSON Lines file.\n\nfilename: Output file path (if ends with .gz or compress=true, writes gzip-compressed)\ndata: An iterable (e.g., Vector, generator) of values (Dict, Array, String, Number, Bool, nothing, etc.)\n\nReturns the filename.\n\nExample\n\nwrite_jsonl(\"out.jsonl\", [Dict(\"a\"=>1), Dict(\"b\"=>2)])\nwrite_jsonl(\"out.jsonl.gz\", (Dict(\"i\"=>i) for i in 1:10^6))\n\n\n\n\n\n","category":"method"},{"location":"#BazerUtils.jl","page":"Home","title":"BazerUtils.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Utility functions for everyday julia.","category":"page"}]
}
